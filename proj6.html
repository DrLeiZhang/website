---
layout: default
---
<img class="about-avatar" src="{{ "/img/object.jpg" | prepend: site.baseurl }}" border="0" alt="[name]" />

<div class="well">
<p><a href="http://www.leizhang.tk/">{Home}</a> <a href="http://www.leizhang.tk/publications%20and%20codes.html">{Publications and Codes}</a> <a href="http://www.leizhang.tk/research.html">{Research}</a> <a href="http://www.leizhang.tk/projects.html"><B>{Projects}</B></a> <a href="http://www.leizhang.tk/professional%20activities.html">{Professional Activities}</a> <a href="http://www.leizhang.tk/people.html">{People}</a></p>
</div>

<div class="well">
<h2>Machine Vision, Image Recognition and Machine Learning</h2>

<h1>[Multi-view Learning for Image Understanding]</a></h1>
<p><B>Paper:</B> </p>
<p>Lei Zhang* and David Zhang, Visual Understanding via Multi-Feature Shared Learning With Global Consistency, IEEE Transactions on Multimedia (<B>T-MM</B>), vol. 18, no. 2, pp. 247-259, 2016. <a href="resources/TMM-2015.pdf">[paper]</a></p>

<p><B>Abstract:</B></p>
<p>Image/video data is usually represented with
multiple visual features. Fusion of multi-source information for
establishing attributes has been widely recognized.
Multi-feature visual recognition has recently received much
attention in multimedia applications. This paper studies visual
understanding via a newly proposed -norm-based multi-feature
shared learning framework, which can simultaneously learn
a global label matrix and multiple sub-classifiers with the
labeled multi-feature data. Additionally, a group graph manifold
regularizer composed of the Laplacian and Hessian graph
is proposed. It can better preserve the manifold structure of
each feature, such that the label prediction power is much
improved through semi-supervised learning with global
label consistency. For convenience, we call the proposed
approach global-label-consistent classifier (GLCC). The
merits of the proposed method include the following: 1) the
manifold structure information of each feature is exploited
in learning, resulting in a more faithful classification owing
to the global label consistency; 2) a group graph manifold
regularizer based on the Laplacian and Hessian regularization is
constructed; and 3) an efficient alternative optimization method
is introduced as a fast solver owing its speed to convex
sub-problems. Experiments on several benchmark visual
datasets—the 17-category Oxford Flower dataset, the challenging
101-category Caltech dataset, the YouTube and Consumer
Videos dataset, and the large-scale NUS-WIDE dataset—have
been used for multimedia understanding. The results
demonstrate that the proposed approach compares favorably with
state-of-the-art algorithms. An extensive experiment using the
deep convolutional activation features also shows the effectiveness
of the proposed approach. </p>

<img class="about-avatar" src="{{ "/img/proj6_1.jpg" | prepend: site.baseurl }}" border="0" alt="[name]" />

<p><B>Approach:</B></p>
<p>The data distribution and decision boundaries. (a) linear classifiers
learned for a three-class problem on labeled data in source domain.
(b) classifiers learned on the source domain do not fit the target domain
due to the change of data distribution. (c) domain adaptation with EDA by
simultaneously learning new classifier and category transformation matrix.
Note that the category transformation denotes the output adaptation with a
matrix Theta.</p>
<img class="about-avatar" src="{{ "/img/proj5_2.jpg" | prepend: site.baseurl }}" border="0" alt="[name]" />
<p>The flowchart of the proposed cross-domain learning framework in multiple views. Specifically, for each domain, the same type of feature is first
extracted. Second, the base classifier is trained on the raw feature of source data. Third, the feature mapping (random projection) is conducted on the both
features of source and target data. Fourth, the EDA based domain adaptation classifier is learned. Finally, the visual categorization task with domain adaptation
is done.</p>
<img class="about-avatar" src="{{ "/img/proj5_3.jpg" | prepend: site.baseurl }}" border="0" alt="[name]" />

<p><B>Experiments:</B></p>
<p>we evaluate our proposed methods EDA
and MvEDA on four datasets: 1) the challenging YouTube &
Consumer videos (SIFT and ST features), 2) the 3DA
Office dataset (SURF feature), 3) the 4DA Extended office
dataset (SURF vs. CNN features), 4) the Bing-Caltech dataset
(Classeme feature). Notably, EDA is termed for single feature
scenarios and MvEDA is termed for multiple features based
application scenarios (e.g., YouTube videos).</p>
<img class="about-avatar" src="{{ "/img/proj5_4.jpg" | prepend: site.baseurl }}" border="0" alt="[name]" />
<img class="about-avatar" src="{{ "/img/proj5_5.jpg" | prepend: site.baseurl }}" border="0" alt="[name]" />

<p>Convergence of EDA for three datasets: (a, d) Consumer & YouTube
Videos; (b, e) 3DA Office dataset: webcam→dslr; (c, f) 4DA Extended Office
dataset: amazon→dslr.</p>
<img class="about-avatar" src="{{ "/img/proj5_6.jpg" | prepend: site.baseurl }}" border="0" alt="[name]" />

</div>



